{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "yLgKmJD8PhVZ"
      },
      "outputs": [],
      "source": [
        "!rm -rf spark-3.1.1-bin-hadoop3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "X6k3DPlpPq0M"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q findspark pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "R1WCldFxPs9k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkbFY_2LPvhw",
        "outputId": "14dbea2e-130e-4c41-d518-e76e07a43da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsoMsGNEPzHg",
        "outputId": "538fa4b7-935b-40e1-d585-e7a7d5691eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: pyspark\n",
            "Version: 3.5.1\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOPDh98WP1Ri",
        "outputId": "ce5da15b-33b1-4920-a041-8a0e1e1ff94e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.25.2)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSI09XYlP3TS",
        "outputId": "07162100-c96c-4fee-c5df-17bbb2e05b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keMWUcUuP46d",
        "outputId": "5d092a5e-4e7a-438f-fbdc-6c21dd5d6c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  242k  100  242k    0     0   713k      0 --:--:-- --:--:-- --:--:--  714k\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!curl -L -o \"/usr/local/lib/python3.10/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.3.2-s_2.11.jar\" https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.1-s_2.12/graphframes-0.8.2-spark3.1-s_2.12.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xKhl00BP60l",
        "outputId": "8491f642-52e4-4903-fa54-8910fdf015e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "activation-1.1.1.jar\n",
            "aircompressor-0.26.jar\n",
            "algebra_2.12-2.0.1.jar\n",
            "annotations-17.0.0.jar\n",
            "antlr4-runtime-4.9.3.jar\n",
            "antlr-runtime-3.5.2.jar\n",
            "aopalliance-repackaged-2.6.1.jar\n",
            "arpack-3.0.3.jar\n",
            "arpack_combined_all-0.1.jar\n",
            "arrow-format-12.0.1.jar\n",
            "arrow-memory-core-12.0.1.jar\n",
            "arrow-memory-netty-12.0.1.jar\n",
            "arrow-vector-12.0.1.jar\n",
            "audience-annotations-0.5.0.jar\n",
            "avro-1.11.2.jar\n",
            "avro-ipc-1.11.2.jar\n",
            "avro-mapred-1.11.2.jar\n",
            "blas-3.0.3.jar\n",
            "bonecp-0.8.0.RELEASE.jar\n",
            "breeze_2.12-2.1.0.jar\n",
            "breeze-macros_2.12-2.1.0.jar\n",
            "cats-kernel_2.12-2.1.1.jar\n",
            "chill_2.12-0.10.0.jar\n",
            "chill-java-0.10.0.jar\n",
            "commons-cli-1.5.0.jar\n",
            "commons-codec-1.16.0.jar\n",
            "commons-collections-3.2.2.jar\n",
            "commons-collections4-4.4.jar\n",
            "commons-compiler-3.1.9.jar\n",
            "commons-compress-1.23.0.jar\n",
            "commons-crypto-1.1.0.jar\n",
            "commons-dbcp-1.4.jar\n",
            "commons-io-2.13.0.jar\n",
            "commons-lang-2.6.jar\n",
            "commons-lang3-3.12.0.jar\n",
            "commons-logging-1.1.3.jar\n",
            "commons-math3-3.6.1.jar\n",
            "commons-pool-1.5.4.jar\n",
            "commons-text-1.10.0.jar\n",
            "compress-lzf-1.1.2.jar\n",
            "curator-client-2.13.0.jar\n",
            "curator-framework-2.13.0.jar\n",
            "curator-recipes-2.13.0.jar\n",
            "datanucleus-api-jdo-4.2.4.jar\n",
            "datanucleus-core-4.1.17.jar\n",
            "datanucleus-rdbms-4.1.19.jar\n",
            "datasketches-java-3.3.0.jar\n",
            "datasketches-memory-2.1.0.jar\n",
            "derby-10.14.2.0.jar\n",
            "dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "flatbuffers-java-1.12.0.jar\n",
            "graphframes-0.8.2-spark3.3.2-s_2.11.jar\n",
            "gson-2.2.4.jar\n",
            "guava-14.0.1.jar\n",
            "hadoop-client-api-3.3.4.jar\n",
            "hadoop-client-runtime-3.3.4.jar\n",
            "hadoop-shaded-guava-1.1.1.jar\n",
            "hadoop-yarn-server-web-proxy-3.3.4.jar\n",
            "HikariCP-2.5.1.jar\n",
            "hive-beeline-2.3.9.jar\n",
            "hive-cli-2.3.9.jar\n",
            "hive-common-2.3.9.jar\n",
            "hive-exec-2.3.9-core.jar\n",
            "hive-jdbc-2.3.9.jar\n",
            "hive-llap-common-2.3.9.jar\n",
            "hive-metastore-2.3.9.jar\n",
            "hive-serde-2.3.9.jar\n",
            "hive-service-rpc-3.1.3.jar\n",
            "hive-shims-0.23-2.3.9.jar\n",
            "hive-shims-2.3.9.jar\n",
            "hive-shims-common-2.3.9.jar\n",
            "hive-shims-scheduler-2.3.9.jar\n",
            "hive-storage-api-2.8.1.jar\n",
            "hk2-api-2.6.1.jar\n",
            "hk2-locator-2.6.1.jar\n",
            "hk2-utils-2.6.1.jar\n",
            "httpclient-4.5.14.jar\n",
            "httpcore-4.4.16.jar\n",
            "istack-commons-runtime-3.0.8.jar\n",
            "ivy-2.5.1.jar\n",
            "jackson-annotations-2.15.2.jar\n",
            "jackson-core-2.15.2.jar\n",
            "jackson-core-asl-1.9.13.jar\n",
            "jackson-databind-2.15.2.jar\n",
            "jackson-dataformat-yaml-2.15.2.jar\n",
            "jackson-datatype-jsr310-2.15.2.jar\n",
            "jackson-mapper-asl-1.9.13.jar\n",
            "jackson-module-scala_2.12-2.15.2.jar\n",
            "jakarta.annotation-api-1.3.5.jar\n",
            "jakarta.inject-2.6.1.jar\n",
            "jakarta.servlet-api-4.0.3.jar\n",
            "jakarta.validation-api-2.0.2.jar\n",
            "jakarta.ws.rs-api-2.1.6.jar\n",
            "jakarta.xml.bind-api-2.3.2.jar\n",
            "janino-3.1.9.jar\n",
            "javassist-3.29.2-GA.jar\n",
            "javax.jdo-3.2.0-m3.jar\n",
            "javolution-5.5.1.jar\n",
            "jaxb-runtime-2.3.2.jar\n",
            "jcl-over-slf4j-2.0.7.jar\n",
            "jdo-api-3.0.1.jar\n",
            "jersey-client-2.40.jar\n",
            "jersey-common-2.40.jar\n",
            "jersey-container-servlet-2.40.jar\n",
            "jersey-container-servlet-core-2.40.jar\n",
            "jersey-hk2-2.40.jar\n",
            "jersey-server-2.40.jar\n",
            "JLargeArrays-1.5.jar\n",
            "jline-2.14.6.jar\n",
            "joda-time-2.12.5.jar\n",
            "jodd-core-3.5.2.jar\n",
            "jpam-1.1.jar\n",
            "json-1.8.jar\n",
            "json4s-ast_2.12-3.7.0-M11.jar\n",
            "json4s-core_2.12-3.7.0-M11.jar\n",
            "json4s-jackson_2.12-3.7.0-M11.jar\n",
            "json4s-scalap_2.12-3.7.0-M11.jar\n",
            "jsr305-3.0.0.jar\n",
            "jta-1.1.jar\n",
            "JTransforms-3.1.jar\n",
            "jul-to-slf4j-2.0.7.jar\n",
            "kryo-shaded-4.0.2.jar\n",
            "kubernetes-client-6.7.2.jar\n",
            "kubernetes-client-api-6.7.2.jar\n",
            "kubernetes-httpclient-okhttp-6.7.2.jar\n",
            "kubernetes-model-admissionregistration-6.7.2.jar\n",
            "kubernetes-model-apiextensions-6.7.2.jar\n",
            "kubernetes-model-apps-6.7.2.jar\n",
            "kubernetes-model-autoscaling-6.7.2.jar\n",
            "kubernetes-model-batch-6.7.2.jar\n",
            "kubernetes-model-certificates-6.7.2.jar\n",
            "kubernetes-model-common-6.7.2.jar\n",
            "kubernetes-model-coordination-6.7.2.jar\n",
            "kubernetes-model-core-6.7.2.jar\n",
            "kubernetes-model-discovery-6.7.2.jar\n",
            "kubernetes-model-events-6.7.2.jar\n",
            "kubernetes-model-extensions-6.7.2.jar\n",
            "kubernetes-model-flowcontrol-6.7.2.jar\n",
            "kubernetes-model-gatewayapi-6.7.2.jar\n",
            "kubernetes-model-metrics-6.7.2.jar\n",
            "kubernetes-model-networking-6.7.2.jar\n",
            "kubernetes-model-node-6.7.2.jar\n",
            "kubernetes-model-policy-6.7.2.jar\n",
            "kubernetes-model-rbac-6.7.2.jar\n",
            "kubernetes-model-resource-6.7.2.jar\n",
            "kubernetes-model-scheduling-6.7.2.jar\n",
            "kubernetes-model-storageclass-6.7.2.jar\n",
            "lapack-3.0.3.jar\n",
            "leveldbjni-all-1.8.jar\n",
            "libfb303-0.9.3.jar\n",
            "libthrift-0.12.0.jar\n",
            "log4j-1.2-api-2.20.0.jar\n",
            "log4j-api-2.20.0.jar\n",
            "log4j-core-2.20.0.jar\n",
            "log4j-slf4j2-impl-2.20.0.jar\n",
            "logging-interceptor-3.12.12.jar\n",
            "lz4-java-1.8.0.jar\n",
            "mesos-1.4.3-shaded-protobuf.jar\n",
            "metrics-core-4.2.19.jar\n",
            "metrics-graphite-4.2.19.jar\n",
            "metrics-jmx-4.2.19.jar\n",
            "metrics-json-4.2.19.jar\n",
            "metrics-jvm-4.2.19.jar\n",
            "minlog-1.3.0.jar\n",
            "netty-all-4.1.96.Final.jar\n",
            "netty-buffer-4.1.96.Final.jar\n",
            "netty-codec-4.1.96.Final.jar\n",
            "netty-codec-http2-4.1.96.Final.jar\n",
            "netty-codec-http-4.1.96.Final.jar\n",
            "netty-codec-socks-4.1.96.Final.jar\n",
            "netty-common-4.1.96.Final.jar\n",
            "netty-handler-4.1.96.Final.jar\n",
            "netty-handler-proxy-4.1.96.Final.jar\n",
            "netty-resolver-4.1.96.Final.jar\n",
            "netty-transport-4.1.96.Final.jar\n",
            "netty-transport-classes-epoll-4.1.96.Final.jar\n",
            "netty-transport-classes-kqueue-4.1.96.Final.jar\n",
            "netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar\n",
            "netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar\n",
            "netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar\n",
            "netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar\n",
            "netty-transport-native-unix-common-4.1.96.Final.jar\n",
            "objenesis-3.3.jar\n",
            "okhttp-3.12.12.jar\n",
            "okio-1.15.0.jar\n",
            "opencsv-2.3.jar\n",
            "orc-core-1.9.2-shaded-protobuf.jar\n",
            "orc-mapreduce-1.9.2-shaded-protobuf.jar\n",
            "orc-shims-1.9.2.jar\n",
            "oro-2.0.8.jar\n",
            "osgi-resource-locator-1.0.3.jar\n",
            "paranamer-2.8.jar\n",
            "parquet-column-1.13.1.jar\n",
            "parquet-common-1.13.1.jar\n",
            "parquet-encoding-1.13.1.jar\n",
            "parquet-format-structures-1.13.1.jar\n",
            "parquet-hadoop-1.13.1.jar\n",
            "parquet-jackson-1.13.1.jar\n",
            "pickle-1.3.jar\n",
            "py4j-0.10.9.7.jar\n",
            "RoaringBitmap-0.9.45.jar\n",
            "rocksdbjni-8.3.2.jar\n",
            "scala-collection-compat_2.12-2.7.0.jar\n",
            "scala-compiler-2.12.18.jar\n",
            "scala-library-2.12.18.jar\n",
            "scala-parser-combinators_2.12-2.3.0.jar\n",
            "scala-reflect-2.12.18.jar\n",
            "scala-xml_2.12-2.1.0.jar\n",
            "shims-0.9.45.jar\n",
            "slf4j-api-2.0.7.jar\n",
            "snakeyaml-2.0.jar\n",
            "snakeyaml-engine-2.6.jar\n",
            "snappy-java-1.1.10.3.jar\n",
            "spark-catalyst_2.12-3.5.1.jar\n",
            "spark-common-utils_2.12-3.5.1.jar\n",
            "spark-core_2.12-3.5.1.jar\n",
            "spark-graphx_2.12-3.5.1.jar\n",
            "spark-hive_2.12-3.5.1.jar\n",
            "spark-hive-thriftserver_2.12-3.5.1.jar\n",
            "spark-kubernetes_2.12-3.5.1.jar\n",
            "spark-kvstore_2.12-3.5.1.jar\n",
            "spark-launcher_2.12-3.5.1.jar\n",
            "spark-mesos_2.12-3.5.1.jar\n",
            "spark-mllib_2.12-3.5.1.jar\n",
            "spark-mllib-local_2.12-3.5.1.jar\n",
            "spark-network-common_2.12-3.5.1.jar\n",
            "spark-network-shuffle_2.12-3.5.1.jar\n",
            "spark-repl_2.12-3.5.1.jar\n",
            "spark-sketch_2.12-3.5.1.jar\n",
            "spark-sql_2.12-3.5.1.jar\n",
            "spark-sql-api_2.12-3.5.1.jar\n",
            "spark-streaming_2.12-3.5.1.jar\n",
            "spark-tags_2.12-3.5.1.jar\n",
            "spark-unsafe_2.12-3.5.1.jar\n",
            "spark-yarn_2.12-3.5.1.jar\n",
            "spire_2.12-0.17.0.jar\n",
            "spire-macros_2.12-0.17.0.jar\n",
            "spire-platform_2.12-0.17.0.jar\n",
            "spire-util_2.12-0.17.0.jar\n",
            "ST4-4.0.4.jar\n",
            "stax-api-1.0.1.jar\n",
            "stream-2.9.6.jar\n",
            "super-csv-2.2.0.jar\n",
            "threeten-extra-1.7.1.jar\n",
            "tink-1.9.0.jar\n",
            "transaction-api-1.1.jar\n",
            "univocity-parsers-2.9.1.jar\n",
            "xbean-asm9-shaded-4.23.jar\n",
            "xz-1.9.jar\n",
            "zjsonpatch-0.3.0.jar\n",
            "zookeeper-3.6.3.jar\n",
            "zookeeper-jute-3.6.3.jar\n",
            "zstd-jni-1.5.5-4.jar\n"
          ]
        }
      ],
      "source": [
        "!ls /usr/local/lib/python3.10/dist-packages/pyspark/jars/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "mkfKBVZXP8oZ"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars\", \"/usr/local/lib/python3.10/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.3.2-s_2.11.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)  # Property used to format output tables better\\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bHxtkcFBQDaS"
      },
      "outputs": [],
      "source": [
        "from graphframes import *\n",
        "from graphframes import GraphFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5n60pLCQQe3",
        "outputId": "7fd7bbb9-1007-47e3-e981-d33fc923bd42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PySpark Version :3.5.1\n",
            "PySpark Version :3.5.1\n"
          ]
        }
      ],
      "source": [
        "print('PySpark Version :'+spark.version)\n",
        "print('PySpark Version :'+spark.sparkContext.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "j6znOeZBQTTo"
      },
      "outputs": [],
      "source": [
        "input_path = \"https://github.com/gauravsharma2/BIG-DATA-GRAPHX\"\n",
        "output_path = \"/content/sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XftDI8cHQcn3"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"BIG DATA ASSIGNMENT_3\").getOrCreate()\n",
        "edges = spark.read.option(\"header\",\"true\").option(\"inferedSchema\",\"true\").option(\"delimiter\",\"\\t\").csv(input_path)\n",
        "edges = edges.select(col(\"FromNodeId\").alias(\"src\"), col(\"ToNodeId\").alias(\"dst\"))\n",
        "display(edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnWyBBCCQuqF"
      },
      "outputs": [],
      "source": [
        "vertices = edges.select(col(\"src\").alias(\"id\")).union(edges.select(col(\"dst\").alias(\"id\"))).distinct()\n",
        "display(vertices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCz1RjMHRUTP"
      },
      "outputs": [],
      "source": [
        "G = GraphFrame(vertices,edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "JsfNMxSRRaj5",
        "outputId": "25720131-b6e1-4f4b-dc9e-993b0b38b374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(id='2565', outDegree=893),\n",
              " Row(id='766', outDegree=773),\n",
              " Row(id='11', outDegree=743),\n",
              " Row(id='457', outDegree=732),\n",
              " Row(id='2688', outDegree=618)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# QUERY : a. Find the top 5 nodes with the highest outdegree and find the count of the number of outgoing edges in each.\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "od = G.outDegrees.orderBy(desc(\"outDegree\")).take(5)\n",
        "odf = spark.createDataFrame(od)\n",
        "odf.write.mode(\"overwrite\").option(\"header\", \"true\").csv(o + \"Query Outputs/OPTION A\")\n",
        "display(od)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "r2oujGDARdrq",
        "outputId": "3c390333-4a18-4d96-bce4-2f6f21db3527"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(id='4037', inDegree=457),\n",
              " Row(id='15', inDegree=361),\n",
              " Row(id='2398', inDegree=340),\n",
              " Row(id='2625', inDegree=331),\n",
              " Row(id='1297', inDegree=309)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# QUERY : b. Find the top 5 nodes with the highest indegree and find the count of the number of incoming edges in each.\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "ind = G.inDegrees.orderBy(desc(\"inDegree\")).take(5)\n",
        "indd = spark.createDataFrame(ind)\n",
        "indd.write.mode(\"overwrite\").option(\"header\", \"true\").csv(o + \"Query Outputs/OPTION B\")\n",
        "display(ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "uXP3P9TqRvAJ",
        "outputId": "06c0b90a-a516-41e9-fbcf-c9c3c719e73e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Row(id='4037', pagerank=32.7613925903508),\n",
              " Row(id='15', pagerank=26.253004957619474),\n",
              " Row(id='6634', pagerank=26.164524434886495),\n",
              " Row(id='2625', pagerank=23.511515933026384),\n",
              " Row(id='2398', pagerank=18.728389390669687)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# QUERY : c. Calculate PageRank for each of the nodes and output the top 5 nodes with the highest PageRank values.\n",
        "# ----------------------------------------------------------------------------------------------------------------\n",
        "pr = G.pageRank(resetProbability=0.15, maxIter=10)\n",
        "pro = pr.vertices.orderBy(desc(\"pagerank\")).select(\"id\", \"pagerank\").distinct()\n",
        "pr5 = pro.orderBy(desc(\"pagerank\")).take(5)\n",
        "prd = spark.createDataFrame(pr5)\n",
        "prd.write.mode(\"overwrite\").option(\"header\", \"true\").csv(o + \"Query Outputs/OPTION C\")\n",
        "display(pr5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "gfFTCiWERyT7",
        "outputId": "bba8d0f6-f67d-4b82-e592-24c603c02803"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Row(component=1, count=3500),\n",
              " Row(component=893353197600, count=3),\n",
              " Row(component=1125281431566, count=3),\n",
              " Row(component=721554505733, count=2),\n",
              " Row(component=987842478110, count=2)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# QUERY : d. Run the connected components algorithm on it and find the top 5 components with the largest number of nodes.\n",
        "# -----------------------------------------------------------------------------------------------------------------------\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\")\n",
        "sp = GraphFrame(vertices, edges.sample(False, 0.1))\n",
        "cc = sp.connectedComponents()\n",
        "cc5 = cc.groupBy(\"component\").count().orderBy(desc(\"count\")).take(5)\n",
        "ccd = spark.createDataFrame(cc5)\n",
        "ccd.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path + \"Query Outputs/OPTION D\")\n",
        "display(cc5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "WcInHsQSR2PH",
        "outputId": "219160ba-bd96-49f5-9761-93d5e4d2b4dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Row(id='2565', count=30940),\n",
              " Row(id='1549', count=22003),\n",
              " Row(id='766', count=18204),\n",
              " Row(id='1166', count=17361),\n",
              " Row(id='2688', count=14220)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# QUERY : e. Run the triangle counts algorithm on each of the vertices and output the top 5 vertices with the largest triangle count.\n",
        "# -----------------------------------------------------------------------------------------------------------------------------------\n",
        "tc = plot.triangleCount()\n",
        "tc5 = tc.select(\"id\", \"count\").distinct().orderBy(desc(\"count\")).take(5)\n",
        "tcd = spark.createDataFrame(tc5)\n",
        "tcd.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path + \"Query Outputs/OPTION E\")\n",
        "display(tc5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPPZkGwER7Ui"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
